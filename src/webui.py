import argparse
import sys

sys.path.insert(0, "/mnt/pa002-28359-vol543625-private/Code/llm_application")
sys.path.insert(0, "/mnt/sfevol775196/sunzeye273/Code/llm_application")
sys.path.insert(0, "/Users/zeyesun/Documents/Code/llm_application")
sys.path.insert(0, "D:\\Code\\llm_application")
import os
import shutil
import gradio as gr
import uuid
import gc
import torch
import re
from typing import List, Dict, Tuple, Union
from tempfile import NamedTemporaryFile
from gradio.inputs import File
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma, VectorStore
from langchain.document_loaders import DirectoryLoader
from langchain.embeddings.base import Embeddings
from langchain.llms.base import LLM
from langchain.text_splitter import CharacterTextSplitter

from src.utils import logger, rmdir, list_dir, print_gpu_utilization
from src.tasks.chatbot import FAQLoader
from src.tasks import Task
from src.apps import init_llm, init_task


# Gradio Settings
block_css = """.importantButton {
    background: linear-gradient(45deg, #7e0570,#5d1c99, #6e00ff) !important;
    border: none !important;
}
.importantButton:hover {
    background: linear-gradient(45deg, #ff00e0,#8500ff, #6e00ff) !important;
    border: none !important;
}"""
webui_title = """
# ğŸ‰LLM Application WebUIğŸ‰
ğŸ‘ [https://github.com/sunzeyeah/llm_application](https://github.com/sunzeyeah/llm_application)
"""
default_theme_args = dict(
    font=["Source Sans Pro", 'ui-sans-serif', 'system-ui', 'sans-serif'],
    font_mono=['IBM Plex Mono', 'ui-monospace', 'Consolas', 'monospace'],
)
flag_csv_logger = gr.CSVLogger()
FLAG_USER_NAME: str = uuid.uuid4().hex
task_list_zh = [
    "æœç´¢å¼•æ“",
    "æ–‡æœ¬æ‘˜è¦",
    "é—®ç­”æœºå™¨äºº",
    "é—²èŠ",
]
task_en_to_zh = {
    "google_search": "æœç´¢å¼•æ“",
    "summarization": "æ–‡æœ¬æ‘˜è¦",
    "chatbot": "é—®ç­”æœºå™¨äºº",
    "chitchat": "é—²èŠ",
}
task_zh_to_en = {
    "æœç´¢å¼•æ“": "google_search",
    "æ–‡æœ¬æ‘˜è¦": "summarization",
    "é—®ç­”æœºå™¨äºº": "chatbot",
    "é—²èŠ": "chitchat",
}

# Global Variables
llm: LLM = None
langchain_task: Task = None
embeddings: Embeddings = None
vector_store: VectorStore = None


def get_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", type=str, default="local", help="openai_api, huggingface_api, custom_api or local,"
                                                                  "specify how to call the llm model")
    parser.add_argument("--task", type=str, default="chatbot", help="google_search, summarization, chatbot, chitchat"
                                                                    "specify the task to perform")
    parser.add_argument("--model_path", type=str,
                        default=f"D:\\Data\\models" if sys.platform == "win32" else \
                            f"/Users/zeyesun/Documents/Data/models" if sys.platform == "darwin" else \
                            f"/mnt/pa002-28359-vol543625-share/LLM-data/checkpoint"
                            # f"/mnt/sfevol775196/sunzeye273/Data/models"
                        )
    parser.add_argument("--model_name", type=str, default="chatglm2-6B")
    parser.add_argument("--language", type=str, default="zh", help="promptä½¿ç”¨çš„è¯­è¨€ï¼Œä¸€èˆ¬ä¸æ¨¡å‹åŒ¹é…")
    parser.add_argument("--verbose", action="store_true", help="æ˜¯å¦è¾“å‡ºä¸­é—´ç»“æœ")
    parser.add_argument("--device_map", type=str, default="auto", help="device map to allocate model,"
                                                                     "[cpu] means cpu"
                                                                     "[0, 1, 2, ...], number means single-card"
                                                                     "[auto, balanced, balanced_low_0] means multi-card")
    # parser.add_argument("--local_rank", type=int, default=0)
    parser.add_argument("--bits", type=int, default=16)
    parser.add_argument("--checkpoint", type=str, default=None)

    # generation config
    parser.add_argument("--max_length", type=int, default=2048)
    parser.add_argument("--max_length_generation", type=int, default=256, help="Maximum number of newly generated tokens")
    parser.add_argument("--do_sample", type=bool, default=False)
    parser.add_argument("--num_return_sequences", type=int, default=1)
    parser.add_argument("--top_k", type=int, default=10)
    parser.add_argument("--top_p", type=float, default=0.9)
    parser.add_argument("--temperature", type=float, default=1.0)
    parser.add_argument("--repetition_penalty", type=float, default=1.0)
    parser.add_argument("--history_length", type=int, default=0, help="Maximum round of history to append to prompt")
    # Task: Summarization
    parser.add_argument("--chunk_size", type=int, default=2048)
    parser.add_argument("--chunk_overlap", type=int, default=0)
    # Task: Search
    parser.add_argument("--serp_api_key", type=str, default=None)
    # Task: ChatBot
    parser.add_argument("--embedding_name", type=str, default="text2vec-large-chinese",
                        help="openai or path to huggingface embedding, specify which embedding method to use")
    parser.add_argument("--vector_dir", type=str,
                        default="D:\\Data\\chatgpt\\output\\embeddings" if sys.platform == "win32" else \
                            "/Users/zeyesun/Documents/Data/chatgpt/output/embeddings" if sys.platform == "darwin" else \
                            "/mnt/pa002-28359-vol543625-private/Data/chatgpt/output/embeddings",
                            # "/mnt/sfevol775196/sunzeye273/Data/chatgpt/output/embeddings",
                        help="æœ¬åœ°çŸ¥è¯†åº“çš„å‘é‡æ–‡ä»¶æ ¹ç›®å½•")
    parser.add_argument("--kb_name", type=str, default="faq", help="çŸ¥è¯†åº“åç§°")
    parser.add_argument("--k", type=int, default=3, help="number of docs to recall for answering")
    parser.add_argument("--search_type", type=str, default="similarity", help="similarity, similarity_score_threshold, mmr"
                                                                              "metrics used to compare document vectors")
    parser.add_argument("--search_threshold", type=float, default=0.7, help="similarity_score_threshold")
    parser.add_argument("--data_dir", type=str, default=None, help="æœ¬åœ°çŸ¥è¯†åº“åŸå§‹æ–‡ä»¶åœ°å€")
    parser.add_argument("--pattern", type=str, default=None, help="æœ¬åœ°çŸ¥è¯†åº“çš„æ–‡ä»¶åpattern")

    args = parser.parse_args()

    return args


def init_embeddings_and_vector_store(vector_dir: str,
                                     data_dir: str = None,
                                     pattern: str = None) -> str:
    global embeddings
    global vector_store
    try:
        # load embedding model
        embedding_device = f"cuda:{torch.cuda.current_device()}" if torch.cuda.is_available() else "cpu"
        embeddings = HuggingFaceEmbeddings(model_name=os.path.join(args.model_path, args.embedding_name),
                                           model_kwargs={'device': embedding_device})
        # make directory (if vector_dir does not exist)
        if not os.path.exists(vector_dir):
            os.mkdir(vector_dir)
        # init or load vector store
        if data_dir is not None:
            # åˆ é™¤åŸembeddingæ–‡ä»¶
            rmdir(vector_dir)
            # åŠ è½½æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰txtç±»å‹çš„æ–‡ä»¶
            loader = DirectoryLoader(data_dir, glob=pattern, loader_cls=FAQLoader, show_progress=True,
                                     use_multithreading=True, max_concurrency=8, loader_kwargs={"encoding": "utf-8"})
            # å°†æ•°æ®è½¬æˆ document å¯¹è±¡ï¼Œæ¯ä¸ªæ–‡ä»¶ä¼šä½œä¸ºä¸€ä¸ª document
            documents = loader.load()
            # åˆå§‹åŒ–åŠ è½½å™¨
            text_splitter = CharacterTextSplitter(chunk_size=args.chunk_size, chunk_overlap=args.chunk_overlap)
            # åˆ‡å‰²åŠ è½½çš„ document
            split_docs = text_splitter.split_documents(documents)
            # å°† document é€šè¿‡ openai çš„ embeddings å¯¹è±¡è®¡ç®— embeddingå‘é‡ä¿¡æ¯å¹¶ä¸´æ—¶å­˜å…¥ Chroma å‘é‡æ•°æ®åº“ï¼Œç”¨äºåç»­åŒ¹é…æŸ¥è¯¢
            vector_store = Chroma.from_documents(split_docs, embeddings, persist_directory=vector_dir)
            # æŒä¹…åŒ–æ•°æ®
            vector_store.persist()
            kb_status = f"""çŸ¥è¯†åº“ï¼š{os.path.basename(vector_dir)}å·²æˆåŠŸæ–°å»ºå¹¶åŠ è½½"""
        else:
            vector_store = Chroma(persist_directory=vector_dir, embedding_function=embeddings)
            kb_status = f"""çŸ¥è¯†åº“ï¼š{os.path.basename(vector_dir)}å·²æˆåŠŸåŠ è½½"""
    except Exception as e:
        kb_status = f"""ã€WARNINGã€‘çŸ¥è¯†åº“ï¼š{os.path.basename(vector_dir)}åŠ è½½å¤±è´¥, {str(e)}"""
        logger.error(kb_status, e)

    return kb_status


def initialize_llm() -> str:
    global llm
    try:
        llm = init_llm(args)
        bits = f"{args.bits}-bit"
        if args.device_map == "cpu":
            loads = "CPU"
        elif args.device_map == "0":
            loads = "å•å¡"
        elif args.device_map == "auto":
            loads = "å¤šå¡ï¼ˆè‡ªåŠ¨è´Ÿè½½ï¼‰"
        elif args.device_map == "balanced":
            loads = "å¤šå¡ï¼ˆå‡è¡¡è´Ÿè½½ï¼‰"
        elif args.device_map == "balanced_low_0":
            loads = "å¤šå¡ï¼ˆå‡è¡¡è´Ÿè½½ï¼Œé™ä½cuda:0ï¼‰"
        else:
            loads = "å¤šå¡ï¼ˆè‡ªå®šä¹‰è´Ÿè½½ï¼‰"
        llm_status = f"""LLMæ¨¡å‹ï¼š{args.model_name}å·²æˆåŠŸåŠ è½½ï¼ŒåŠ è½½æ¨¡å¼ï¼š{loads} + {bits}"""
    except torch.cuda.OutOfMemoryError as e:
        llm_status = f"""ã€WARNINGã€‘åŠ è½½LLMæ¨¡å‹ï¼š{args.model_name} æ—¶å‘ç”ŸCUDA out of memoryï¼Œè¯·å¼€å¯å¤šå¡æˆ–è€…ä½¿ç”¨8-bitå’Œ4-bit"""
        logger.error(llm_status, e)
    except Exception as e:
        llm_status = f"""ã€WARNINGã€‘LLMæ¨¡å‹ï¼š{args.model_name}åŠ è½½å¤±è´¥, {str(e)}\nè¯·åˆ°é¡µé¢å·¦ä¸Šè§’"æ¨¡å‹é…ç½®"é€‰é¡¹å¡ä¸­é‡æ–°é€‰æ‹©åç‚¹å‡»"åŠ è½½æ¨¡å‹"æŒ‰é’®"""
        logger.error(llm_status, e)

    return llm_status


def initialize_task() -> str:
    global langchain_task
    try:
        langchain_task = init_task(args, llm, embeddings)
        task_status = f"""ä»»åŠ¡ï¼š{task_en_to_zh[args.task]}å·²æˆåŠŸåŠ è½½ï¼Œå¯ä»¥å¼€å§‹å¯¹è¯"""
    except Exception as e:
        if args.task == "chitchat":
            task_status = f"""ä»»åŠ¡ï¼š{task_en_to_zh[args.task]}å·²æˆåŠŸåŠ è½½ï¼Œå¯ä»¥å¼€å§‹å¯¹è¯"""
        elif args.task == "google_search" and args.serp_api_key is None:
            task_status = f"""ã€WARNINGã€‘ä»»åŠ¡ï¼š{task_en_to_zh[args.task]}é»˜è®¤ä½¿ç”¨Googleï¼Œéœ€è¦SERP_API_KEYï¼Œè¯·åœ¨å³ä¾§è¾“å…¥æ¡†å†…è¿›è¡Œè¾“å…¥"""
            logger.warning(task_status)
        else:
            task_status = f"""ã€WARNINGã€‘ä»»åŠ¡ï¼š{task_en_to_zh[args.task]}åŠ è½½å¤±è´¥, {str(e)}"""
            logger.error(task_status, e)

    return task_status


def update_model_params(
        llm_model: str,
        embedding_model: str,
        kb_name: str,
        device_map: str,
        bits: int,
        checkpoint: str,
        max_length_generation: int,
        do_sample: bool,
        top_p: float,
        temperature: float,
        repetition_penalty: float,
        history_length: int,
        history: List[List[str]]) -> List[List[str]]:
    global args
    global llm
    global embeddings
    global vector_store
    args.device_map = device_map
    args.bits = bits
    args.checkpoint = checkpoint
    args.max_length_generation = max_length_generation
    args.do_sample = do_sample
    args.top_p = top_p
    args.temperature = temperature
    args.repetition_penalty = repetition_penalty
    args.history_length = history_length

    # release occupied GPU memory
    if torch.cuda.is_available():
        # print_gpu_utilization("before gpu release", args.local_rank, False)
        try:
            del llm.pipeline.model
            llm.pipeline.model = None
        except AttributeError:
            pass
        try:
            del llm.pipeline.tokenizer
            llm.pipeline.tokenizer = None
        except AttributeError:
            pass
        try:
            del embeddings
            embeddings = None
        except NameError:
            pass
        try:
            del vector_store
            vector_store = None
        except NameError:
            pass
        gc.collect()
        # with torch.cuda.device(f"cuda:{args.local_rank}"):
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
        # cuda.select_device(args.local_rank)
        # cuda.close()
        # print_gpu_utilization("after gpu release", args.local_rank, False)

    try:
        args.model_name = llm_model
        args.embedding_name = embedding_model
        # re-init embeddings and vector store
        kb_status = init_embeddings_and_vector_store(vector_dir=os.path.join(args.vector_dir, kb_name))
        logger.debug(kb_status)
        # re-init llm
        llm_status = initialize_llm()
        llm_status = kb_status + "ï¼Œ" + llm_status
        # llm.pipeline._forward_params.update({"do_sample": do_sample, "top_p": top_p,
        #                                      "temperature": temperature, "repetition_penalty": repetition_penalty})
        # llm_status = f"LLMå‚æ•°å·²æ›´æ–°ï¼Œdo_sample={do_sample}, top_p={top_p}, temperature={temperature}, " \
        #              f"repetition_penalty={repetition_penalty}"
        logger.debug(llm_status)
    except Exception as e:
        llm_status = f"""ã€WARNINGã€‘LLMæ¨¡å‹å‚æ•°æ›´æ–°å¤±è´¥, {str(e)}"""
        logger.error(llm_status, e)
    return history + [[None, llm_status]]


def get_kb_list() -> List[str]:
    try:
        kb_names = list_dir(args.vector_dir)
        kb_names.sort()
    except Exception as e:
        logger.error("Failed to list kb", e)
        kb_names = []
    return kb_names


def refresh_kb_list() -> Tuple[Dict, Dict]:
    return gr.update(choices=get_kb_list()), gr.update(choices=get_kb_list())


def add_kb(kb_name: str, chatbot: List[List[str]]) -> Tuple[Dict, List[List[str]], Dict]:
    # select_kb, chatbot, kb_delete
    if kb_name is None or kb_name.strip() == "":
        kb_status = "ã€WARNINGã€‘çŸ¥è¯†åº“åç§°ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°å¡«å†™çŸ¥è¯†åº“åç§°"
        chatbot = chatbot + [[None, kb_status]]
        return gr.update(visible=True), \
               chatbot, \
               gr.update(visible=True)
    elif kb_name in get_kb_list():
        kb_status = "ã€WARNINGã€‘ä¸å·²æœ‰çŸ¥è¯†åº“åç§°å†²çªï¼Œè¯·é‡æ–°é€‰æ‹©å…¶ä»–åç§°åæäº¤"
        chatbot = chatbot + [[None, kb_status]]
        return gr.update(visible=True), \
               chatbot, \
               gr.update(visible=True)
    else:
        data_dir, pattern = kb_name.rsplit(os.sep, maxsplit=1)
        kb_name = os.path.basename(data_dir)
        kb_path = os.path.join(args.vector_dir, kb_name)
        kb_status = init_embeddings_and_vector_store(kb_path, data_dir, pattern)
        chatbot = chatbot + [[None, kb_status]]
        return gr.update(visible=True, choices=get_kb_list(), value=kb_name), \
               chatbot, \
               gr.update(visible=True, choices=get_kb_list())


def delete_kb(kb_to_delte: str, current_kb: str, chatbot: List[List[str]]) -> Tuple[Dict, List[List[str]], Dict]:
    try:
        assert kb_to_delte != current_kb
        # åˆ é™¤çŸ¥è¯†åº“å‘é‡æ–‡ä»¶åœ°å€
        shutil.rmtree(os.path.join(args.vector_dir, kb_to_delte))
        kb_status = f"æˆåŠŸåˆ é™¤çŸ¥è¯†åº“ï¼š{kb_to_delte}"
        logger.info(kb_status)
        # # é‡æ–°åŠ è½½å‘é‡æ–‡ä»¶ (if necessary)
        # kb_list = get_kb_list()
        # if len(kb_list) > 0:
        #     kb_first = kb_list[0]
        #     kb_path = os.path.join(args.vector_dir, kb_first)
        #     kb_status += "\n" + init_embeddings_and_vector_store(kb_path)
        chatbot = chatbot + [[None, kb_status]]
        return gr.update(choices=get_kb_list(), value=current_kb), \
               chatbot, \
               gr.update(choices=get_kb_list(), value=current_kb)
    except AssertionError as e:
        kb_status = f"ã€WARNINGã€‘å¾…åˆ é™¤çŸ¥è¯†åº“ä¸å½“å‰ä½¿ç”¨ä¸­çŸ¥è¯†åº“ç›¸åŒï¼Œæ— æ³•åˆ é™¤ï¼Œè¯·æ¢æˆå…¶ä»–çŸ¥è¯†åº“"
        logger.error(kb_status, e)
        chatbot = chatbot + [[None, kb_status]]
        return gr.update(visible=True), \
               chatbot, \
               gr.update(visible=True)
    except Exception as e:
        kb_status = f"ã€WARNINGã€‘åˆ é™¤çŸ¥è¯†åº“ï¼š{kb_to_delte}å¤±è´¥, {str(e)}"
        logger.error(kb_status, e)
        chatbot = chatbot + [[None, kb_status]]
        return gr.update(visible=True), \
               chatbot, \
               gr.update(visible=True)


def reselect_kb(kb_name: str) -> Dict:
    return gr.update(value=kb_name)


def change_kb(kb_name: str, history: List[List[str]]) -> List[List[str]]:
    kb_path = os.path.join(args.vector_dir, kb_name)
    kb_status = init_embeddings_and_vector_store(kb_path)

    return history + [[None, kb_status]]


def load_summarization_files(files: Union[File, List[File]],
                             history: List[List[str]]) -> Tuple[List[str], List[List[str]]]:
    # files, chatbot
    if not isinstance(files, list):
        files = [files]

    loaded_files = [file.name for file in files]
    # global split_documents
    # for file in files:
    # try:
    #     # å¯¼å…¥æ–‡æœ¬
    #     loader = UnstructuredFileLoader(file.name)
    #     # å°†æ–‡æœ¬è½¬æˆ Document å¯¹è±¡
    #     document = loader.load()
    #     logger.debug(f'document length: {len(document)}')
    #     # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
    #     text_splitter = RecursiveCharacterTextSplitter(
    #         chunk_size=args.chunk_size,
    #         chunk_overlap=args.chunk_overlap
    #     )
    #     # åˆ‡åˆ†æ–‡æœ¬
    #     split_documents.extend(text_splitter.split_documents(document))
    #     loaded_files.append(file.name)
    # except Exception as e:
    #     logger.warning(f"Failed to load {file.name}")

    if len(loaded_files):
        file_status = f"å·²æ·»åŠ å¦‚ä¸‹æ–‡ä»¶ï¼š {'ã€'.join([os.path.basename(f) for f in loaded_files])} "
    else:
        file_status = "ã€WARNINGã€‘æ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œè¯·é‡æ–°ä¸Šä¼ æ–‡ä»¶"
    logger.info(file_status)

    return loaded_files, \
           history + [[None, file_status]]


def change_task(task: str, history: List[List[str]]) -> Tuple[Dict, Dict, Dict, Dict, List[List[str]]]:
    global args
    args.task = task_zh_to_en[task]
    task_status = initialize_task()
    if task == "é—®ç­”æœºå™¨äºº":
        return gr.update(visible=True), \
               gr.update(visible=True), \
               gr.update(visible=False), \
               gr.update(visible=False), \
               history + [[None, task_status]]
    elif task == "æ–‡æœ¬æ‘˜è¦":
        return gr.update(visible=False), \
               gr.update(visible=False), \
               gr.update(visible=True), \
               gr.update(visible=False), \
               history + [[None, task_status]]
    elif task == "æœç´¢å¼•æ“":
        return gr.update(visible=False), \
               gr.update(visible=False), \
               gr.update(visible=False), \
               gr.update(visible=True), \
               history + [[None, task_status]]
    else:
        return gr.update(visible=False), \
               gr.update(visible=False), \
               gr.update(visible=False), \
               gr.update(visible=False), \
               history + [[None, task_status]]


def init_search(api_key: str, history: List[List[str]]) -> List[List[str]]:
    global args
    args.serp_api_key = api_key
    assert args.task == 'google_search'
    task_status = initialize_task()

    return history + [[None, task_status]]


def update_kb_params(score: float,
                     k: int,
                     chunk_size: int,
                     history: List[List[str]]) -> List[List[str]]:
    global args
    args.search_threshold = score
    args.k = k
    args.chunk_size = chunk_size
    status = f"""çŸ¥è¯†åº“å‚æ•°å·²æ›´æ–°ï¼Œå¬å›é˜ˆå€¼={score}, å¬å›æ•°é‡={k}, å•æ®µå†…å®¹çš„æœ€å¤§é•¿åº¦(chunk_size)={chunk_size}"""
    return history + [[None, status]]


@torch.no_grad()
def get_answer(task: str,
               history: List[List[str]],
               query: str = None,
               files: List[NamedTemporaryFile] = None) -> None:
    try:
        if task == "æœç´¢å¼•æ“":
            result = langchain_task(prompt=query)
            for resp in [result]:
                reply = f"é—®ï¼š{query}\n\nç­”ï¼š{resp}"
                history.append([None, reply])
                yield history, ""
        elif task == "æ–‡æœ¬æ‘˜è¦":
            for file in files:
                resp = langchain_task(input_file=file.name, chunk_size=args.chunk_size,
                                      chunk_overlap=args.chunk_overlap)
                reply = f"æ‘˜è¦ï¼š{resp}"
                history.append([None, reply])
                yield history, ""
        elif task == "é—®ç­”æœºå™¨äºº":
            result = langchain_task(query=query, search_type=args.search_type, k=args.k)
            for resp in [result]:
                source = [
                    f"<details>" \
                    f"<summary>å‡ºå¤„ï¼š[{i + 1}] {doc.page_content}</summary>\n" \
                    f"{doc.metadata['answer']}\n" \
                    f"</details>"
                    for i, doc in enumerate(resp["source_documents"])
                ]
                reply = "\n\n".join([f"é—®ï¼š{query}", f"ç­”ï¼š{result['result']}"] + source)
                history.append([None, reply])
                yield history, ""
        else:
            # only select the most recent chitchat history, search or chatbot history is not selected
            pattern = r"(.*?)é—®ï¼š[\s]*(.*?)ç­”ï¼š[\s]*(.*)"
            for idx in range(len(history)-1, -1, -1):
                _, text = history[idx]
                if "é—²èŠå·²æˆåŠŸåŠ è½½" in text:
                    break
            else:
                idx = len(history)
            dialog_history = []
            for i in range(idx, len(history)):
                _, text = history[i]
                match = re.search(pattern, text, re.DOTALL)
                if match:
                    old_query = match.group(2).strip()
                    old_reponse = match.group(3).strip()
                    dialog_history.append((old_query, old_reponse))
            logger.debug(f"dialog_history: {dialog_history}")
            result = llm(query, history=dialog_history)
            for resp in [result]:
                reply = f"é—®ï¼š{query}\n\nç­”ï¼š{resp}"
                history[-1][-1] += "\n\n" + reply
                yield history, ""
    except torch.cuda.OutOfMemoryError as e:
        reply = f"""ã€WARNINGã€‘è®¡ç®—ç­”æ¡ˆæ—¶å‘ç”ŸCUDA out of memoryï¼Œè¯·å¼€å¯å¤šå¡æˆ–è€…ä½¿ç”¨8-bitå’Œ4-bit"""
        history.append([None, reply])
        yield history, ""
    except Exception as e:
        logger.error("è·å–ç­”æ¡ˆå¤±è´¥", e)
        reply = str(e)
        history.append([None, reply])
        yield history, ""
    logger.info(f"flagging: username={FLAG_USER_NAME}, task={task}, query={query}, history={history}")
    flag_csv_logger.flag([task, query, history], username=FLAG_USER_NAME)


# LangChain and LLM Params
args = get_parser()
init_message = f"""æ¬¢è¿ä½¿ç”¨ LLM Application Web UIï¼

è¯·åœ¨å³ä¾§åˆ‡æ¢ä»»åŠ¡ï¼Œç›®å‰æ”¯æŒ{len(task_list_zh)}ç±»ï¼š{" ".join([f"({i + 1}) {t}" for i, t in enumerate(task_list_zh)])}

å½“å‰ä»»åŠ¡ï¼š{task_en_to_zh[args.task]}
å½“å‰LLMæ¨¡å‹ï¼š{args.model_name}
å½“å‰embeddingæ¨¡å‹ï¼š{args.embedding_name}
å½“å‰çŸ¥è¯†åº“ï¼š{args.kb_name}
"""
llm_model_list = [
    "chatglm2-6B",
    "chatglm2-6B-int4",
    "chatglm-6B",
    "vicuna-7B-v1.1",
    "baichuan-13B-chat",
    "bloomz-560M",
    "llama-7B",
    "llama-13B",
    "llama2-7B-chat",
    "llama2-13B-chat",
]
embedding_model_list = [
    "text2vec-large-chinese",
]

# åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹ï¼ˆLLM, Embeddings, Chainç­‰ï¼‰
llm_status = initialize_llm()
task_status = initialize_task()

# Gradioé…ç½®
with gr.Blocks(css=block_css, theme=gr.themes.Default(**default_theme_args)) as demo:
    task_status = gr.State(task_status)
    llm_status = gr.State(llm_status)
    kb_status = gr.State(args.kb_name)
    file_status = gr.State("")
    gr.Markdown(webui_title)
    with gr.Tab("å¯¹è¯"):
        with gr.Row():
            with gr.Column(scale=10):
                chatbot = gr.Chatbot([[None, init_message], [None, llm_status.value]],
                                     elem_id="chat-box",
                                     show_label=False).style(height=750)
                query = gr.Textbox(show_label=False,
                                   placeholder="è¯·è¾“å…¥æé—®å†…å®¹ï¼ŒæŒ‰å›è½¦è¿›è¡Œæäº¤").style(container=False)
            with gr.Column(scale=5):
                task = gr.Radio(task_list_zh, label="è¯·é€‰æ‹©ä»»åŠ¡", value=task_en_to_zh[args.task])
                kb_params = gr.Accordion("ã€é—®ç­”æœºå™¨äººã€‘å‚æ•°è®¾å®š", visible=task_en_to_zh[args.task] == "é—®ç­”æœºå™¨äºº")
                kb_setting = gr.Accordion("ã€é—®ç­”æœºå™¨äººã€‘ä¿®æ”¹çŸ¥è¯†åº“", visible=task_en_to_zh[args.task] == "é—®ç­”æœºå™¨äºº")
                summarization_setting = gr.Accordion("ã€æ–‡æœ¬æ‘˜è¦ã€‘ä¸Šä¼ æ–‡ä»¶", visible=task_en_to_zh[args.task] == "æ–‡æœ¬æ‘˜è¦")
                search_setting = gr.Accordion("ã€æœç´¢å¼•æ“ã€‘API KEY", visible=task_en_to_zh[args.task] == "æœç´¢å¼•æ“")
                task.change(fn=change_task,
                            inputs=[task, chatbot],
                            outputs=[kb_params, kb_setting, summarization_setting, search_setting, chatbot])
                with kb_params:
                    search_threshold_slider = gr.Slider(0.0, 1.0, value=args.search_threshold, step=0.1,
                                                        label="å¬å›é˜ˆå€¼ï¼šç›¸ä¼¼åº¦è¶…è¿‡è¯¥å€¼çš„documentæ‰ä¼šè¢«å¬å›", interactive=True)
                    # search_threshold_number = gr.Number(value=args.search_threshold, minimum=0.0, maximum=1.0, precision=1,
                    #                                     label="å¬å›é˜ˆå€¼ï¼šç›¸ä¼¼åº¦è¶…è¿‡è¯¥å€¼çš„documentæ‰ä¼šè¢«å¬å›", interactive=True)
                    # k_number = gr.Number(value=args.k, precision=0,
                    #                      label="å¬å›æ•°é‡ï¼šæ¯æ¬¡æœ€å¤šå¬å›çš„documentæ•°é‡", interactive=True)
                    k_slider = gr.Slider(1, 10, value=args.k, step=1,
                                         label="å¬å›æ•°é‡ï¼šæ¯æ¬¡æœ€å¤šå¬å›çš„documentæ•°é‡", interactive=True)
                    # chunk_conent = gr.Checkbox(value=False,
                    #                            label="æ˜¯å¦å¯ç”¨ä¸Šä¸‹æ–‡å…³è”",
                    #                            interactive=True)
                    # chunk_size_number = gr.Number(value=args.chunk_size, precision=0, minimum=64, maximum=4096,
                    #                               label="æœ€å¤§é•¿åº¦ï¼šå•æ®µå†…å®¹çš„æœ€å¤§é•¿åº¦ï¼Œè¶…è¿‡è¯¥å€¼ä¼šè¢«åˆ‡åˆ†ä¸ºä¸åŒdocument", interactive=True)
                    chunk_size_slider = gr.Slider(64, 4096, value=args.chunk_size, step=1,
                                                  label="æœ€å¤§é•¿åº¦ï¼šå•æ®µå†…å®¹çš„æœ€å¤§é•¿åº¦ï¼Œè¶…è¿‡è¯¥å€¼ä¼šè¢«åˆ‡åˆ†ä¸ºä¸åŒdocument", interactive=True)
                    update_kb_params_button = gr.Button("æ›´æ–°çŸ¥è¯†åº“å‚æ•°")
                    update_kb_params_button.click(fn=update_kb_params,
                                                  inputs=[search_threshold_slider, k_slider, chunk_size_slider,
                                                          chatbot],
                                                  outputs=chatbot)
                with kb_setting:
                    kb_select_dropdown = gr.Dropdown(get_kb_list(),
                                                     label="è¯·é€‰æ‹©è¦åŠ è½½çš„çŸ¥è¯†åº“",
                                                     interactive=True,
                                                     value=args.kb_name)
                    kb_select_button = gr.Button("åˆ‡æ¢çŸ¥è¯†åº“")
                    kb_add_textbox = gr.Textbox(label='è¯·è¾“å…¥æ–°å¢çŸ¥è¯†åº“çš„åŸå§‹æ–‡ä»¶åœ°å€',
                                                info='è·¯å¾„æ ¼å¼ï¼špath/{kb_name}/*.jsonlï¼Œæ–‡ä»¶ä¸­æ¯è¡Œçš„æ ¼å¼ï¼š{"prompt": "", "label": ""}ï¼‰',
                                                lines=1,
                                                interactive=True)
                    kb_add_button = gr.Button(value="æ–°å¢çŸ¥è¯†åº“")
                    kb_delete_dropdown = gr.Dropdown(get_kb_list(),
                                                     label="è¯·é€‰æ‹©è¦åˆ é™¤çš„çŸ¥è¯†åº“",
                                                     interactive=True,
                                                     value=get_kb_list()[0] if len(get_kb_list()) > 0 else None,
                                                     visible=True)
                    kb_delete_button = gr.Button("åˆ é™¤çŸ¥è¯†åº“", visible=True)
                    # select_kb.change(fn=reselect_kb,
                    #                  inputs=select_kb,
                    #                  outputs=select_kb)
                    kb_select_button.click(fn=change_kb,
                                           inputs=[kb_select_dropdown, chatbot],
                                           outputs=chatbot)
                    kb_add_button.click(fn=add_kb,
                                        inputs=[kb_add_textbox, chatbot],
                                        outputs=[kb_select_dropdown, chatbot, kb_delete_dropdown])
                    # delete_kb.change(fn=reselect_kb,
                    #                  inputs=delete_kb,
                    #                  outputs=delete_kb)
                    kb_delete_button.click(fn=delete_kb,
                                           inputs=[kb_delete_dropdown, kb_select_dropdown, chatbot],
                                           outputs=[kb_select_dropdown, chatbot, kb_delete_dropdown])
                    flag_csv_logger.setup([task, query, chatbot], "flagged")
                with summarization_setting:
                    file2kb = gr.Column()
                    with file2kb:
                        #     # load_kb = gr.Button("åŠ è½½çŸ¥è¯†åº“")
                        #     gr.Markdown("å‘çŸ¥è¯†åº“ä¸­æ·»åŠ æ–‡ä»¶")
                        #     sentence_size = gr.Number(value=SENTENCE_SIZE, precision=0,
                        #                               label="æ–‡æœ¬å…¥åº“åˆ†å¥é•¿åº¦é™åˆ¶",
                        #                               interactive=True, visible=True)
                        with gr.Tab("ä¸Šä¼ æ–‡ä»¶"):
                            files = gr.File(label="æ·»åŠ æ–‡ä»¶",
                                            file_types=['.txt', '.md', '.doc', '.docx', '.pdf', '.tsv', '.json', ".csv",
                                                        "jsonl"],
                                            file_count="multiple",
                                            show_label=False)
                            load_file_button = gr.Button("ä¸Šä¼ æ–‡ä»¶")
                        # with gr.Tab("ä¸Šä¼ æ–‡ä»¶å¤¹"):
                        #     folder_files = gr.File(label="æ·»åŠ æ–‡ä»¶å¤¹",
                        #                            file_count="directory",
                        #                            show_label=False)
                        #     load_folder_button = gr.Button("ä¸Šä¼ æ–‡ä»¶å¤¹å¹¶åŠ è½½çŸ¥è¯†åº“")
                        # with gr.Tab("åˆ é™¤çŸ¥è¯†åº“"):
                        #     files_to_delete = gr.CheckboxGroup(choices=[],
                        #                                        label="åˆ é™¤æ•´ä¸ªçŸ¥è¯†åº“å‘é‡æ–‡ä»¶",
                        #                                        interactive=True)
                        #     delete_file_button = gr.Button("åˆ é™¤æ•´ä¸ªçŸ¥è¯†åº“å‘é‡æ–‡ä»¶")
                    load_file_button.click(load_summarization_files,
                                           show_progress=True,
                                           inputs=[files, chatbot],
                                           outputs=[files, chatbot])
                    # load_folder_button.click(get_vector_store,
                    #                          show_progress=True,
                    #                          inputs=[select_kb, folder_files, sentence_size, chatbot, kb_add,
                    #                                  kb_add],
                    #                          outputs=[kb_path, folder_files, chatbot, files_to_delete], )
                    # delete_file_button.click(delete_file,
                    #                          show_progress=True,
                    #                          inputs=[select_kb, files_to_delete, chatbot],
                    #                          outputs=[files_to_delete, chatbot])
                    # flag_csv_logger.setup([task, files, chatbot], "flagged")
                with search_setting:
                    serp_api_key_textbox = gr.Textbox(label="è¯·è¾“å…¥SERP API KEY",
                                                      lines=1,
                                                      interactive=True)
                    serp_api_key_button = gr.Button(value="ç¡®è®¤")
                    serp_api_key_button.click(fn=init_search,
                                              inputs=[serp_api_key_textbox, chatbot],
                                              outputs=chatbot)
                    # flag_csv_logger.setup([task, query, serp_api_key_textbox, chatbot], "flagged")
                query.submit(get_answer,
                             [task, chatbot, query, files],
                             [chatbot, query])
    with gr.Tab("æ¨¡å‹é…ç½®"):
        llm_model_dropdown = gr.Dropdown(llm_model_list,
                                         label="LLM æ¨¡å‹",
                                         value=args.model_name,
                                         interactive=True,
                                         visible=True)
        embedding_model_dropdown = gr.Dropdown(embedding_model_list,
                                               label="Embedding æ¨¡å‹",
                                               value=args.embedding_name,
                                               interactive=True,
                                               visible=True)
        bits_radio = gr.Radio([4, 8, 16, 32], value=args.bits,
                              label="æ¨¡å‹åŠ è½½bitæ•°",
                              interactive=True)
        device_map_radio = gr.Radio(["cpu", "0", "auto", "balanced", "balanced_low_0", "custom"],
                                    value=args.device_map,
                                    label="æ˜¯å¦ä½¿ç”¨GPUã€æ˜¯å¦å¼€å¯å¤šå¡ä»¥åŠå¤šå¡è´Ÿè½½ç®¡ç†",
                                    info="cpu-CPUï¼Œ0-å•å¡ï¼Œauto-å¤šå¡ï¼ˆè‡ªåŠ¨è´Ÿè½½ï¼‰ï¼Œbalanced-å¤šå¡ï¼ˆå‡è¡¡è´Ÿè½½ï¼‰ï¼Œbalanced_low_0-å¤šå¡ï¼ˆå‡è¡¡è´Ÿè½½ï¼Œé™ä½cuda:0ï¼‰ï¼Œcustom-å¤šå¡ï¼ˆè‡ªå®šä¹‰è´Ÿè½½ï¼‰",
                                    interactive=True)
        checkpoint_textbox = gr.Textbox(value=args.checkpoint, label="æ¨¡å‹checkpointæ–‡ä»¶å", interactive=True)
        max_length_generation_slider = gr.Slider(8, 4096, value=args.max_length_generation, step=1,
                                                 label="ç”Ÿæˆå‚æ•°ï¼šmax_new_tokens",
                                                 interactive=True)
        do_sample_checkbox = gr.Checkbox(args.do_sample,
                                         label="ç”Ÿæˆå‚æ•°ï¼šdo_sample",
                                         interactive=True)
        top_p_slider = gr.Slider(0.0, 1.0, value=args.top_p, step=0.01,
                                 label="ç”Ÿæˆå‚æ•°ï¼štop_p", interactive=True)
        temperature_slider = gr.Slider(0.0, 5.0, value=args.temperature, step=0.1,
                                       label="ç”Ÿæˆå‚æ•°ï¼štemperature", interactive=True)
        repetition_penalty_slider = gr.Slider(0.0, 5.0, value=args.repetition_penalty, step=0.1,
                                              label="ç”Ÿæˆå‚æ•°ï¼šrepetition_penalty", interactive=True)
        history_length_slider = gr.Slider(0, 10, value=args.history_length, step=1,
                                          label="æœ€å¤§å†å²è½®æ•°ï¼šhistory_length",
                                          info="ç›®å‰ä»…ç”¨äºé—²èŠä»»åŠ¡å’ŒChatGLMç±»æ¨¡å‹", interactive=True)
        update_model_params_button = gr.Button("æ›´æ–°å‚æ•°å¹¶é‡æ–°åŠ è½½æ¨¡å‹")
        update_model_params_button.click(update_model_params,
                                         show_progress=True,
                                         inputs=[llm_model_dropdown, embedding_model_dropdown, kb_select_dropdown, device_map_radio,
                                                 bits_radio, checkpoint_textbox, max_length_generation_slider, do_sample_checkbox, top_p_slider,
                                                 temperature_slider, repetition_penalty_slider, history_length_slider, chatbot],
                                         outputs=chatbot)

    demo.load(
        fn=refresh_kb_list,
        inputs=None,
        outputs=[kb_select_dropdown, kb_delete_dropdown],
        queue=True,
    )

(demo
 .queue(concurrency_count=8)
 .launch(server_name='0.0.0.0',
         server_port=7860,
         show_api=False,
         share=False,
         inbrowser=False))
