import sys

sys.path.insert(0, "/mnt/pa002-28359-vol543625-private/Code/llm_application")
sys.path.insert(0, "/Users/zeyesun/Documents/Code/llm_application")
sys.path.insert(0, "D:\\Code\\llm_application")
import os
import shutil
import gradio as gr
import uuid
import torch
from typing import List, Dict, Tuple, Union
from gradio.inputs import File
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma, VectorStore
from langchain.document_loaders import DirectoryLoader
from langchain.embeddings.base import Embeddings
from langchain.llms.base import LLM
from langchain.text_splitter import CharacterTextSplitter

from src.utils import logger, rmdir, list_dir
from src.tasks.chatbot import FAQLoader
from src.tasks import Task
from src.apps import init_llm, init_task


class dotdict(dict):
    __getattr__ = dict.get
    __setattr__ = dict.__setitem__
    __delattr__ = dict.__delitem__


# Gradio Settings
block_css = """.importantButton {
    background: linear-gradient(45deg, #7e0570,#5d1c99, #6e00ff) !important;
    border: none !important;
}
.importantButton:hover {
    background: linear-gradient(45deg, #ff00e0,#8500ff, #6e00ff) !important;
    border: none !important;
}"""
webui_title = """
# ğŸ‰LLM Application WebUIğŸ‰
ğŸ‘ [https://github.com/sunzeyeah/llm_application](https://github.com/sunzeyeah/llm_application)
"""
default_theme_args = dict(
    font=["Source Sans Pro", 'ui-sans-serif', 'system-ui', 'sans-serif'],
    font_mono=['IBM Plex Mono', 'ui-monospace', 'Consolas', 'monospace'],
)
flag_csv_logger = gr.CSVLogger()
FLAG_USER_NAME: str = uuid.uuid4().hex
task_list_zh = [
    "æœç´¢å¼•æ“",
    "æ–‡æœ¬æ‘˜è¦",
    "çŸ¥è¯†åº“é—®ç­”",
]
task_en_to_zh = {
    "google_search": "æœç´¢å¼•æ“",
    "summarization": "æ–‡æœ¬æ‘˜è¦",
    "chatbot": "çŸ¥è¯†åº“é—®ç­”",
}
task_zh_to_en = {
    "æœç´¢å¼•æ“": "google_search",
    "æ–‡æœ¬æ‘˜è¦": "summarization",
    "çŸ¥è¯†åº“é—®ç­”": "chatbot",
}
default_task = "chatbot"
default_llm_model = "bloomz-560M"
default_embedding_name = "text2vec-large-chinese"
default_kb_name = "test"
init_message = f"""æ¬¢è¿ä½¿ç”¨ LLM Application Web UIï¼

è¯·åœ¨å³ä¾§åˆ‡æ¢ä»»åŠ¡ï¼Œç›®å‰æ”¯æŒ{len(task_list_zh)}ç±»ï¼š{" ".join([f"({i+1}) {t}" for i, t in enumerate(task_list_zh)])}

å½“å‰ä»»åŠ¡ï¼š{task_en_to_zh[default_task]}
å½“å‰LLMæ¨¡å‹ï¼š{default_llm_model}
å½“å‰embeddingæ¨¡å‹ï¼š{default_embedding_name}
å½“å‰çŸ¥è¯†åº“ï¼š{default_kb_name}
"""

# LangChain and LLM Params
args = {
    "mode": "local",
    "task": default_task,
    # "model_name": f"/mnt/pa002-28359-vol543625-share/LLM/checkpoint/{default_llm_model}",
    "model_name": f"/Users/zeyesun/Documents/Data/models/{default_llm_model}",
    # "model_name": f"D:\\Data\\models\\{default_llm_model}",
    "local_rank": 0,
    "checkpoint": None,
    "bits": 16,
    "max_length_generation": 256,
    "do_sample": False,
    "top_p": 0.9,
    "temperature": 0.9,
    "repetition_penalty": 1.0,
    "chunk_size": 1024,
    "chunk_overlap": 0,
    # "vector_dir": "/mnt/pa002-28359-vol543625-private/Data/chatgpt/output/embeddings",
    # "embedding_name": f"/mnt/pa002-28359-vol543625-share/LLM/checkpoint/{default_embedding_name}",
    "vector_dir": "/Users/zeyesun/Documents/Data/chatgpt/output/embeddings",
    "embedding_name": f"/Users/zeyesun/Documents/Data/models/{default_embedding_name}",
    # "vector_dir": "D:\\Data\\chatgpt\\output\\embeddings",
    # "embedding_name": f"D:\\Data\\models\\{default_embedding_name}",
    "kb_name": default_kb_name,
    "search_type": "similarity",
    "k": 5,
    "search_threshold": 0.7,
    "serp_api_key": None,
}
args = dotdict(args)
llm_model_list = [
    "chatglm2-6B",
    "bloomz-560M",
]
embedding_model_list = [
    "text2vec-large-chinese",
]

# Global Variables
llm: LLM = None
langchain_task: Task = None
embeddings: Embeddings = None
vector_store: VectorStore = None


def init_embeddings_and_vector_store(vector_dir: str,
                                     data_dir: str = None,
                                     pattern: str = None) -> str:
    global embeddings
    global vector_store
    try:
        embedding_device = f"cuda:{args.local_rank}" if torch.cuda.is_available() else "cpu"
        embeddings = HuggingFaceEmbeddings(model_name=args.embedding_name,
                                           model_kwargs={'device': embedding_device})
        if data_dir is not None:
            # åˆ é™¤åŸembeddingæ–‡ä»¶
            rmdir(vector_dir)
            # åŠ è½½æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰txtç±»å‹çš„æ–‡ä»¶
            loader = DirectoryLoader(data_dir, glob=pattern, loader_cls=FAQLoader, show_progress=True,
                                     use_multithreading=True, max_concurrency=8, loader_kwargs={"encoding": "utf-8"})
            # å°†æ•°æ®è½¬æˆ document å¯¹è±¡ï¼Œæ¯ä¸ªæ–‡ä»¶ä¼šä½œä¸ºä¸€ä¸ª document
            documents = loader.load()
            # åˆå§‹åŒ–åŠ è½½å™¨
            text_splitter = CharacterTextSplitter(chunk_size=args.chunk_size, chunk_overlap=args.chunk_overlap)
            # åˆ‡å‰²åŠ è½½çš„ document
            split_docs = text_splitter.split_documents(documents)
            # å°† document é€šè¿‡ openai çš„ embeddings å¯¹è±¡è®¡ç®— embeddingå‘é‡ä¿¡æ¯å¹¶ä¸´æ—¶å­˜å…¥ Chroma å‘é‡æ•°æ®åº“ï¼Œç”¨äºåç»­åŒ¹é…æŸ¥è¯¢
            vector_store = Chroma.from_documents(split_docs, embeddings, persist_directory=vector_dir)
            # æŒä¹…åŒ–æ•°æ®
            vector_store.persist()
            kb_status = f"""çŸ¥è¯†åº“ï¼š{os.path.basename(vector_dir)}å·²æˆåŠŸæ–°å»ºå¹¶åŠ è½½"""
        else:
            vector_store = Chroma(persist_directory=vector_dir, embedding_function=embeddings)
            kb_status = f"""çŸ¥è¯†åº“ï¼š{os.path.basename(vector_dir)}å·²æˆåŠŸåŠ è½½"""
    except Exception as e:
        logger.error(e)
        kb_status = f"""ã€WARNINGã€‘çŸ¥è¯†åº“ï¼š{os.path.basename(vector_dir)}åŠ è½½å¤±è´¥"""
        logger.warning(kb_status)

    return kb_status


def initialize_llm() -> str:
    global llm
    try:
        llm = init_llm(args)
        llm_status = f"""LLMæ¨¡å‹ï¼š{os.path.basename(args.model_name)}å·²æˆåŠŸåŠ è½½"""
    except Exception as e:
        logger.error(e)
        llm_status = f"""ã€WARNINGã€‘LLMæ¨¡å‹ï¼š{os.path.basename(args.model_name)}åŠ è½½å¤±è´¥ï¼Œè¯·åˆ°é¡µé¢å·¦ä¸Šè§’"æ¨¡å‹é…ç½®"é€‰é¡¹å¡ä¸­é‡æ–°é€‰æ‹©åç‚¹å‡»"åŠ è½½æ¨¡å‹"æŒ‰é’®"""
        logger.warning(llm_status)

    return llm_status


def initialize_task() -> str:
    global langchain_task
    try:
        langchain_task = init_task(args, llm, embeddings)
        task_status = f"""ä»»åŠ¡ï¼š{task_en_to_zh[args.task]}å·²æˆåŠŸåŠ è½½ï¼Œå¯ä»¥å¼€å§‹å¯¹è¯"""
    except Exception as e:
        logger.error(e)
        if args.task == "google_search" and args.serp_api_key is None:
            task_status = f"""ã€WARNINGã€‘ä»»åŠ¡ï¼š{task_en_to_zh[args.task]}é»˜è®¤ä½¿ç”¨Googleï¼Œéœ€è¦SERP_API_KEYï¼Œè¯·åœ¨å³ä¾§è¾“å…¥æ¡†å†…è¿›è¡Œè¾“å…¥"""
        else:
            task_status = f"""ã€WARNINGã€‘ä»»åŠ¡ï¼š{task_en_to_zh[args.task]}åŠ è½½å¤±è´¥"""
        logger.warning(task_status)

    return task_status


def reinit_model(llm_model: str,
                 embedding_model: str,
                 kb_name: str,
                 # llm_history_len, no_remote_model, use_ptuning_v2, use_lora,
                 history: List[List[str]]) -> List[List[str]]:
    global args
    try:
        model_dir = os.sep.join(args.model_name.split(os.sep)[:-1])
        args.model_name = os.path.join(model_dir, llm_model)
        args.embedding_name = os.path.join(model_dir, embedding_model)
        kb_status = init_embeddings_and_vector_store(vector_dir=os.path.join(args.vector_dir, kb_name))
        logger.debug(kb_status)
        initialize_llm()
        llm_status = f"""LLMæ¨¡å‹æˆåŠŸæ›´æ¢ä¸º{llm_model}ï¼ŒEmbeddingæ¨¡å‹æˆåŠŸæ›´æ¢ä¸º{embedding_model}ï¼ŒæˆåŠŸåŠ è½½çŸ¥è¯†åº“ï¼š{kb_name}"""
        logger.debug(llm_status)
    except Exception as e:
        logger.error(e)
        llm_status = f"""ã€WARNINGã€‘LLMæ¨¡å‹æœªæ›´æ¢ä¸º{llm_model}ï¼ŒEmbeddingæ¨¡å‹æœªæ›´æ¢ä¸º{embedding_model}ï¼Œ
        è¯·åˆ°é¡µé¢å·¦ä¸Šè§’"æ¨¡å‹é…ç½®"é€‰é¡¹å¡ä¸­é‡æ–°é€‰æ‹©åç‚¹å‡»"åŠ è½½æ¨¡å‹"æŒ‰é’®"""
        logger.warning(llm_status)
    return history + [[None, llm_status]]


def get_kb_list() -> List[str]:
    try:
        kb_names = list_dir(args.vector_dir)
        kb_names.sort()
    except Exception as e:
        logger.warning("Failed to list kb", e)
        kb_names = []
    return kb_names


def refresh_kb_list() -> Dict:
    return gr.update(choices=get_kb_list())


def add_kb(kb_name: str, chatbot: List[List[str]]) -> Tuple[Dict, List[List[str]], Dict]:
    # select_kb, chatbot, kb_delete
    if kb_name is None or kb_name.strip() == "":
        kb_status = "ã€WARNINGã€‘çŸ¥è¯†åº“åç§°ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°å¡«å†™çŸ¥è¯†åº“åç§°"
        chatbot = chatbot + [[None, kb_status]]
        return gr.update(visible=True), \
               chatbot, \
               gr.update(visible=True)
    elif kb_name in get_kb_list():
        kb_status = "ã€WARNINGã€‘ä¸å·²æœ‰çŸ¥è¯†åº“åç§°å†²çªï¼Œè¯·é‡æ–°é€‰æ‹©å…¶ä»–åç§°åæäº¤"
        chatbot = chatbot + [[None, kb_status]]
        return gr.update(visible=True), \
               chatbot, \
               gr.update(visible=True)
    else:
        data_dir, pattern = kb_name.rsplit(os.sep, maxsplit=1)
        kb_name = os.path.basename(data_dir)
        kb_path = os.path.join(args.vector_dir, kb_name)
        kb_status = init_embeddings_and_vector_store(kb_path, data_dir, pattern)
        chatbot = chatbot + [[None, kb_status]]
        return gr.update(visible=True, choices=get_kb_list(), value=kb_name), \
               chatbot, \
               gr.update(visible=True, choices=get_kb_list())


def delete_kb(kb_name: str, chatbot: List[List[str]]) -> Tuple[Dict, List[List[str]], Dict]:
    try:
        shutil.rmtree(os.path.join(args.vector_dir, kb_name))
        kb_status = f"æˆåŠŸåˆ é™¤çŸ¥è¯†åº“ï¼š{kb_name}"
        logger.info(kb_status)
        chatbot = chatbot + [[None, kb_status]]
        return gr.update(choices=get_kb_list()), \
               chatbot, \
               gr.update(choices=get_kb_list())
    except Exception as e:
        kb_status = f"ã€WARNINGã€‘åˆ é™¤çŸ¥è¯†åº“ï¼š{kb_name}å¤±è´¥"
        logger.error(kb_status, e)
        chatbot = chatbot + [[None, kb_status]]
        return gr.update(visible=True), \
               chatbot, \
               gr.update(visible=True)


def reselect_kb(kb_name: str) -> Dict:
    return gr.update(value=kb_name)


def change_kb(kb_name: str, history: List[List[str]]) -> List[List[str]]:
    kb_path = os.path.join(args.vector_dir, kb_name)
    kb_status = init_embeddings_and_vector_store(kb_path)

    return history + [[None, kb_status]]


def load_summarization_files(files: Union[File, List[File]],
                             history: List[List[str]]) -> Tuple[List[str], List[List[str]]]:
    # files, chatbot
    if not isinstance(files, list):
        files = [files]

    loaded_files = [file.name for file in files]
    # global split_documents
    # for file in files:
    # try:
    #     # å¯¼å…¥æ–‡æœ¬
    #     loader = UnstructuredFileLoader(file.name)
    #     # å°†æ–‡æœ¬è½¬æˆ Document å¯¹è±¡
    #     document = loader.load()
    #     logger.debug(f'document length: {len(document)}')
    #     # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
    #     text_splitter = RecursiveCharacterTextSplitter(
    #         chunk_size=args.chunk_size,
    #         chunk_overlap=args.chunk_overlap
    #     )
    #     # åˆ‡åˆ†æ–‡æœ¬
    #     split_documents.extend(text_splitter.split_documents(document))
    #     loaded_files.append(file.name)
    # except Exception as e:
    #     logger.warning(f"Failed to load {file.name}")

    if len(loaded_files):
        file_status = f"å·²æ·»åŠ å¦‚ä¸‹æ–‡ä»¶ï¼š {'ã€'.join([os.path.basename(f) for f in loaded_files])} "
    else:
        file_status = "ã€WARNINGã€‘æ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œè¯·é‡æ–°ä¸Šä¼ æ–‡ä»¶"
    logger.info(file_status)

    return loaded_files, \
           history + [[None, file_status]]


def change_task(task: str, history: List[List[str]]) -> Tuple[Dict, Dict, Dict, Dict, List[List[str]]]:
    global args
    args.task = task_zh_to_en[task]
    task_status = initialize_task()
    if task == "çŸ¥è¯†åº“é—®ç­”":
        return gr.update(visible=True), \
               gr.update(visible=True), \
               gr.update(visible=False), \
               gr.update(visible=False), \
               history + [[None, task_status]]
    elif task == "æ–‡æœ¬æ‘˜è¦":
        return gr.update(visible=False), \
               gr.update(visible=False), \
               gr.update(visible=True), \
               gr.update(visible=False), \
               history + [[None, task_status]]
    elif task == "æœç´¢å¼•æ“":
        return gr.update(visible=False), \
               gr.update(visible=False), \
               gr.update(visible=False), \
               gr.update(visible=True), \
               history + [[None, task_status]]
    else:
        return gr.update(visible=False), \
               gr.update(visible=False), \
               gr.update(visible=False), \
               gr.update(visible=False), \
               history + [[None, task_status]]


def init_search(api_key: str, history: List[List[str]]) -> List[List[str]]:
    global args
    args.serp_api_key = api_key
    assert args.task == 'google_search'
    task_status = initialize_task()

    return history + [[None, task_status]]


def get_answer(task: str,
               query: str,
               files: List[str],
               history: List[List[str]]) -> None:
    # logger.info(f"[get_answer] history: {history}")
    if task == "æœç´¢å¼•æ“":
        result = langchain_task(prompt=query)
        for resp in [result]:
            reply = "\n\n"
            reply += resp
            history[-1][-1] += reply
            yield history, ""
    elif task == "æ–‡æœ¬æ‘˜è¦":
        for file in files:
            resp = langchain_task(input_file=file, chunk_size=args.chunk_size,
                                  chunk_overlap=args.chunk_overlap)
            reply = "\n\n"
            reply += resp
            history[-1][-1] += reply
            yield history, ""
    elif task == "çŸ¥è¯†åº“é—®ç­”":
        result = langchain_task(query=query, search_type=args.search_type, k=args.k)
        # logger.info(f"query: {query}, result: {result}")
        for resp in [result]:
            reply = "\n\n"
            source = [
                f"<details>" \
                f"<summary>å‡ºå¤„ï¼š[{i + 1}] {doc.page_content}</summary>\n" \
                f"{doc.metadata['answer']}\n" \
                f"</details>"
                for i, doc in enumerate(resp["source_documents"])
            ]
            reply += "\n\n".join([f"é—®ï¼š{query}", f"ç­”ï¼š{result['result']}"] + source)
            history[-1][-1] += reply
            yield history, ""
    else:
        result = llm(query)
        for resp in [result]:
            reply = "\n\n"
            reply += resp
            history[-1][-1] = reply
            yield history, ""
    logger.info(f"flagging: username={FLAG_USER_NAME}, task={task}, query={query}, history={history}")
    flag_csv_logger.flag([query, history, task], username=FLAG_USER_NAME)


# åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹ï¼ˆLLM, Embeddings, Chainç­‰ï¼‰
llm_status = initialize_llm()
task_status = initialize_task()

# Gradioé…ç½®
with gr.Blocks(css=block_css, theme=gr.themes.Default(**default_theme_args)) as demo:
    task_status = gr.State(task_status)
    llm_status = gr.State(llm_status)
    kb_status = gr.State(default_kb_name)
    file_status = gr.State("")
    gr.Markdown(webui_title)
    with gr.Tab("å¯¹è¯"):
        with gr.Row():
            with gr.Column(scale=10):
                chatbot = gr.Chatbot([[None, init_message], [None, llm_status.value]],
                                     elem_id="chat-box",
                                     show_label=False).style(height=750)
                query = gr.Textbox(show_label=False,
                                   placeholder="è¯·è¾“å…¥æé—®å†…å®¹ï¼ŒæŒ‰å›è½¦è¿›è¡Œæäº¤").style(container=False)
            with gr.Column(scale=5):
                task = gr.Radio(task_list_zh, label="è¯·é€‰æ‹©ä»»åŠ¡", value=task_en_to_zh[default_task])
                kb_params = gr.Accordion("ã€çŸ¥è¯†åº“é—®ç­”ã€‘å‚æ•°è®¾å®š", visible=task_en_to_zh[default_task] == "çŸ¥è¯†åº“é—®ç­”")
                kb_setting = gr.Accordion("ã€çŸ¥è¯†åº“é—®ç­”ã€‘ä¿®æ”¹çŸ¥è¯†åº“", visible=task_en_to_zh[default_task] == "çŸ¥è¯†åº“é—®ç­”")
                summarization_setting = gr.Accordion("ã€æ–‡æœ¬æ‘˜è¦ã€‘ä¸Šä¼ æ–‡ä»¶", visible=task_en_to_zh[default_task] == "æ–‡æœ¬æ‘˜è¦")
                search_setting = gr.Accordion("ã€æœç´¢å¼•æ“ã€‘API KEY", visible=task_en_to_zh[default_task] == "æœç´¢å¼•æ“")
                task.change(fn=change_task,
                            inputs=[task, chatbot],
                            outputs=[kb_params, kb_setting, summarization_setting, search_setting, chatbot])
                with kb_params:
                    args.search_threshold = gr.Number(value=args.search_threshold,
                                                      label="å¬å›é˜ˆå€¼ï¼šç›¸ä¼¼åº¦è¶…è¿‡è¯¥å€¼çš„documentæ‰ä¼šè¢«å¬å›",
                                                      precision=1,
                                                      interactive=True)
                    args.k = gr.Number(value=args.k, precision=0,
                                       label="å¬å›æ•°é‡ï¼šæ¯æ¬¡æœ€å¤šå¬å›çš„documentæ•°é‡", interactive=True)
                    # chunk_conent = gr.Checkbox(value=False,
                    #                            label="æ˜¯å¦å¯ç”¨ä¸Šä¸‹æ–‡å…³è”",
                    #                            interactive=True)
                    args.chunk_size = gr.Number(value=args.chunk_size, precision=0,
                                                label="æœ€å¤§é•¿åº¦ï¼šå•æ®µå†…å®¹çš„æœ€å¤§é•¿åº¦ï¼Œè¶…è¿‡è¯¥å€¼ä¼šè¢«åˆ‡åˆ†ä¸ºä¸åŒdocument",
                                                interactive=True)
                    # chunk_conent.change(fn=change_chunk_conent,
                    #                     inputs=[chunk_conent, gr.Textbox(value="chunk_conent", visible=False), chatbot],
                    #                     outputs=[chunk_sizes, chatbot])
                with kb_setting:
                    kb_select_dropdown = gr.Dropdown(get_kb_list(),
                                                     label="è¯·é€‰æ‹©è¦åŠ è½½çš„çŸ¥è¯†åº“",
                                                     interactive=True,
                                                     value=get_kb_list()[0] if len(get_kb_list()) > 0 else None)
                    kb_select_button = gr.Button("åˆ‡æ¢çŸ¥è¯†åº“")
                    kb_add_textbox = gr.Textbox(label="è¯·è¾“å…¥æ–°å¢çŸ¥è¯†åº“çš„åŸå§‹æ–‡ä»¶åœ°å€ï¼ˆå¦‚ï¼špath/*.txtï¼‰",
                                                lines=1,
                                                interactive=True,
                                                visible=True)
                    kb_add_button = gr.Button(value="æ–°å¢çŸ¥è¯†åº“", visible=True)

                    kb_delete_dropdown = gr.Dropdown(get_kb_list(),
                                                     label="è¯·é€‰æ‹©è¦åˆ é™¤çš„çŸ¥è¯†åº“",
                                                     interactive=True,
                                                     value=get_kb_list()[0] if len(get_kb_list()) > 0 else None)
                    kb_delete_button = gr.Button("åˆ é™¤çŸ¥è¯†åº“", visible=True)
                    # select_kb.change(fn=reselect_kb,
                    #                  inputs=select_kb,
                    #                  outputs=select_kb)
                    kb_select_button.click(fn=change_kb,
                                           inputs=[kb_select_dropdown, chatbot],
                                           outputs=chatbot)
                    kb_add_button.click(fn=add_kb,
                                        inputs=[kb_add_textbox, chatbot],
                                        outputs=[kb_select_dropdown, chatbot, kb_delete_dropdown])
                    # delete_kb.change(fn=reselect_kb,
                    #                  inputs=delete_kb,
                    #                  outputs=delete_kb)
                    kb_delete_button.click(fn=delete_kb,
                                           inputs=[kb_delete_dropdown, chatbot],
                                           outputs=[kb_select_dropdown, chatbot, kb_delete_dropdown])
                    flag_csv_logger.setup([task, query, chatbot], "flagged")
                with summarization_setting:
                    file2kb = gr.Column(visible=True)
                    with file2kb:
                        #     # load_kb = gr.Button("åŠ è½½çŸ¥è¯†åº“")
                        #     gr.Markdown("å‘çŸ¥è¯†åº“ä¸­æ·»åŠ æ–‡ä»¶")
                        #     sentence_size = gr.Number(value=SENTENCE_SIZE, precision=0,
                        #                               label="æ–‡æœ¬å…¥åº“åˆ†å¥é•¿åº¦é™åˆ¶",
                        #                               interactive=True, visible=True)
                        with gr.Tab("ä¸Šä¼ æ–‡ä»¶"):
                            files = gr.File(label="æ·»åŠ æ–‡ä»¶",
                                            file_types=['.txt', '.md', '.doc', '.docx', '.pdf', '.tsv', '.json', ".csv",
                                                        "jsonl"],
                                            file_count="multiple",
                                            show_label=False)
                            load_file_button = gr.Button("ä¸Šä¼ æ–‡ä»¶")
                        # with gr.Tab("ä¸Šä¼ æ–‡ä»¶å¤¹"):
                        #     folder_files = gr.File(label="æ·»åŠ æ–‡ä»¶å¤¹",
                        #                            file_count="directory",
                        #                            show_label=False)
                        #     load_folder_button = gr.Button("ä¸Šä¼ æ–‡ä»¶å¤¹å¹¶åŠ è½½çŸ¥è¯†åº“")
                        # with gr.Tab("åˆ é™¤çŸ¥è¯†åº“"):
                        #     files_to_delete = gr.CheckboxGroup(choices=[],
                        #                                        label="åˆ é™¤æ•´ä¸ªçŸ¥è¯†åº“å‘é‡æ–‡ä»¶",
                        #                                        interactive=True)
                        #     delete_file_button = gr.Button("åˆ é™¤æ•´ä¸ªçŸ¥è¯†åº“å‘é‡æ–‡ä»¶")
                    load_file_button.click(load_summarization_files,
                                           show_progress=True,
                                           inputs=[files, chatbot],
                                           outputs=[files, chatbot])
                    # load_folder_button.click(get_vector_store,
                    #                          show_progress=True,
                    #                          inputs=[select_kb, folder_files, sentence_size, chatbot, kb_add,
                    #                                  kb_add],
                    #                          outputs=[kb_path, folder_files, chatbot, files_to_delete], )
                    # delete_file_button.click(delete_file,
                    #                          show_progress=True,
                    #                          inputs=[select_kb, files_to_delete, chatbot],
                    #                          outputs=[files_to_delete, chatbot])
                    flag_csv_logger.setup([task, files, chatbot], "flagged")
                with search_setting:
                    serp_api_key_textbox = gr.Textbox(label="è¯·è¾“å…¥SERP API KEY",
                                                      lines=1,
                                                      interactive=True,
                                                      visible=True)
                    serp_api_key_button = gr.Button(value="ç¡®è®¤", visible=True)
                    serp_api_key_button.click(fn=init_search,
                                              inputs=[serp_api_key_textbox, chatbot],
                                              outputs=chatbot)
                    flag_csv_logger.setup([task, query, serp_api_key_textbox, chatbot], "flagged")
                query.submit(get_answer,
                             [task, query, files, chatbot],
                             [chatbot, query])
    # with gr.Tab("çŸ¥è¯†åº“æµ‹è¯• Beta"):
    #     with gr.Row():
    #         with gr.Column(scale=10):
    #             chatbot = gr.Chatbot([[None, knowledge_base_test_mode_info]],
    #                                  elem_id="chat-box",
    #                                  show_label=False).style(height=750)
    #             query = gr.Textbox(show_label=False,
    #                                placeholder="è¯·è¾“å…¥æé—®å†…å®¹ï¼ŒæŒ‰å›è½¦è¿›è¡Œæäº¤").style(container=False)
    #         with gr.Column(scale=5):
    #             mode = gr.Radio(["çŸ¥è¯†åº“æµ‹è¯•"],  # "çŸ¥è¯†åº“é—®ç­”",
    #                             label="è¯·é€‰æ‹©ä½¿ç”¨æ¨¡å¼",
    #                             value="çŸ¥è¯†åº“æµ‹è¯•",
    #                             visible=False)
    #             knowledge_set = gr.Accordion("çŸ¥è¯†åº“è®¾å®š", visible=True)
    #             kb_setting = gr.Accordion("é…ç½®çŸ¥è¯†åº“", visible=True)
    #             mode.change(fn=change_mode,
    #                         inputs=[mode, chatbot],
    #                         outputs=[kb_setting, knowledge_set, chatbot])
    #             with knowledge_set:
    #                 score_threshold = gr.Number(value=VECTOR_SEARCH_SCORE_THRESHOLD,
    #                                             label="çŸ¥è¯†ç›¸å…³åº¦ Score é˜ˆå€¼ï¼Œåˆ†å€¼è¶Šä½åŒ¹é…åº¦è¶Šé«˜",
    #                                             precision=0,
    #                                             interactive=True)
    #                 vector_search_top_k = gr.Number(value=VECTOR_SEARCH_TOP_K, precision=0,
    #                                                 label="è·å–çŸ¥è¯†åº“å†…å®¹æ¡æ•°", interactive=True)
    #                 chunk_conent = gr.Checkbox(value=False,
    #                                            label="æ˜¯å¦å¯ç”¨ä¸Šä¸‹æ–‡å…³è”",
    #                                            interactive=True)
    #                 chunk_sizes = gr.Number(value=CHUNK_SIZE, precision=0,
    #                                         label="åŒ¹é…å•æ®µå†…å®¹çš„è¿æ¥ä¸Šä¸‹æ–‡åæœ€å¤§é•¿åº¦",
    #                                         interactive=True, visible=False)
    #                 chunk_conent.change(fn=change_chunk_conent,
    #                                     inputs=[chunk_conent, gr.Textbox(value="chunk_conent", visible=False), chatbot],
    #                                     outputs=[chunk_sizes, chatbot])
    #             with kb_setting:
    #                 kb_refresh = gr.Button("æ›´æ–°å·²æœ‰çŸ¥è¯†åº“é€‰é¡¹")
    #                 select_kb_test = gr.Dropdown(get_kb_list(),
    #                                              label="è¯·é€‰æ‹©è¦åŠ è½½çš„çŸ¥è¯†åº“",
    #                                              interactive=True,
    #                                              value=get_kb_list()[0] if len(get_kb_list()) > 0 else None)
    #                 kb_name = gr.Textbox(label="è¯·è¾“å…¥æ–°å»ºçŸ¥è¯†åº“åç§°ï¼Œå½“å‰çŸ¥è¯†åº“å‘½åæš‚ä¸æ”¯æŒä¸­æ–‡",
    #                                      lines=1,
    #                                      interactive=True,
    #                                      visible=True)
    #                 kb_add = gr.Button(value="æ·»åŠ è‡³çŸ¥è¯†åº“é€‰é¡¹", visible=True)
    #                 file2kb = gr.Column(visible=False)
    #                 with file2kb:
    #                     # load_kb = gr.Button("åŠ è½½çŸ¥è¯†åº“")
    #                     gr.Markdown("å‘çŸ¥è¯†åº“ä¸­æ·»åŠ å•æ¡å†…å®¹æˆ–æ–‡ä»¶")
    #                     sentence_size = gr.Number(value=SENTENCE_SIZE, precision=0,
    #                                               label="æ–‡æœ¬å…¥åº“åˆ†å¥é•¿åº¦é™åˆ¶",
    #                                               interactive=True, visible=True)
    #                     with gr.Tab("ä¸Šä¼ æ–‡ä»¶"):
    #                         files = gr.File(label="æ·»åŠ æ–‡ä»¶",
    #                                         file_types=['.txt', '.md', '.docx', '.pdf'],
    #                                         file_count="multiple",
    #                                         show_label=False
    #                                         )
    #                         load_file_button = gr.Button("ä¸Šä¼ æ–‡ä»¶å¹¶åŠ è½½çŸ¥è¯†åº“")
    #                     with gr.Tab("ä¸Šä¼ æ–‡ä»¶å¤¹"):
    #                         folder_files = gr.File(label="æ·»åŠ æ–‡ä»¶",
    #                                                # file_types=['.txt', '.md', '.docx', '.pdf'],
    #                                                file_count="directory",
    #                                                show_label=False)
    #                         load_folder_button = gr.Button("ä¸Šä¼ æ–‡ä»¶å¤¹å¹¶åŠ è½½çŸ¥è¯†åº“")
    #                     with gr.Tab("æ·»åŠ å•æ¡å†…å®¹"):
    #                         one_title = gr.Textbox(label="æ ‡é¢˜", placeholder="è¯·è¾“å…¥è¦æ·»åŠ å•æ¡æ®µè½çš„æ ‡é¢˜", lines=1)
    #                         one_conent = gr.Textbox(label="å†…å®¹", placeholder="è¯·è¾“å…¥è¦æ·»åŠ å•æ¡æ®µè½çš„å†…å®¹", lines=5)
    #                         one_content_segmentation = gr.Checkbox(value=True, label="ç¦æ­¢å†…å®¹åˆ†å¥å…¥åº“",
    #                                                                interactive=True)
    #                         load_conent_button = gr.Button("æ·»åŠ å†…å®¹å¹¶åŠ è½½çŸ¥è¯†åº“")
    #                 # å°†ä¸Šä¼ çš„æ–‡ä»¶ä¿å­˜åˆ°contentæ–‡ä»¶å¤¹ä¸‹,å¹¶æ›´æ–°ä¸‹æ‹‰æ¡†
    #                 kb_refresh.click(fn=refresh_kb_list,
    #                                  inputs=[],
    #                                  outputs=select_kb_test)
    #                 kb_add.click(fn=add_kb_name,
    #                              inputs=[kb_name, chatbot],
    #                              outputs=[select_kb_test, kb_name, kb_add, file2kb, chatbot])
    #                 select_kb_test.change(fn=change_kb_name_input,
    #                                       inputs=[select_kb_test, chatbot],
    #                                       outputs=[kb_name, kb_add, file2kb, kb_path, chatbot])
    #                 load_file_button.click(get_vector_store,
    #                                        show_progress=True,
    #                                        inputs=[select_kb_test, files, sentence_size, chatbot, kb_add, kb_add],
    #                                        outputs=[kb_path, files, chatbot], )
    #                 load_folder_button.click(get_vector_store,
    #                                          show_progress=True,
    #                                          inputs=[select_kb_test, folder_files, sentence_size, chatbot, kb_add,
    #                                                  kb_add],
    #                                          outputs=[kb_path, folder_files, chatbot], )
    #                 load_conent_button.click(get_vector_store,
    #                                          show_progress=True,
    #                                          inputs=[select_kb_test, one_title, sentence_size, chatbot,
    #                                                  one_conent, one_content_segmentation],
    #                                          outputs=[kb_path, files, chatbot], )
    #                 flag_csv_logger.setup([query, kb_path, chatbot, mode], "flagged")
    #                 query.submit(get_answer,
    #                              [query, kb_path, chatbot, mode, score_threshold, vector_search_top_k, chunk_conent,
    #                               chunk_sizes],
    #                              [chatbot, query])
    with gr.Tab("æ¨¡å‹é…ç½®"):
        llm_model = gr.Radio(llm_model_list,
                             label="LLM æ¨¡å‹",
                             value=default_llm_model,
                             interactive=True)
        embedding_model = gr.Radio(embedding_model_list,
                                   label="Embedding æ¨¡å‹",
                                   value=default_embedding_name,
                                   interactive=True)
        # no_remote_model = gr.Checkbox(shared.LoaderCheckPoint.no_remote_model,
        #                               label="åŠ è½½æœ¬åœ°æ¨¡å‹",
        #                               interactive=True)
        # llm_history_len = gr.Slider(0, 10,
        #                             value=LLM_HISTORY_LEN,
        #                             step=1,
        #                             label="LLM å¯¹è¯è½®æ•°",
        #                             interactive=True)
        # use_ptuning_v2 = gr.Checkbox(USE_PTUNING_V2,
        #                              label="ä½¿ç”¨p-tuning-v2å¾®è°ƒè¿‡çš„æ¨¡å‹",
        #                              interactive=True)
        args.do_sample = gr.Checkbox(args.do_sample,
                                     label="ç”Ÿæˆå‚æ•°ï¼šdo_sample",
                                     interactive=True)
        args.top_p = gr.Slider(0.0, 1.0, value=args.top_p, step=0.1,
                               label="ç”Ÿæˆå‚æ•°ï¼štop_p", interactive=True)
        args.temperature = gr.Slider(0.0, 5.0, value=args.temperature, step=0.1,
                                     label="ç”Ÿæˆå‚æ•°ï¼štemperature", interactive=True)
        args.repetition_penalty = gr.Slider(0.0, 5.0, value=args.repetition_penalty, step=0.1,
                                            label="ç”Ÿæˆå‚æ•°ï¼šrepetition_penalty", interactive=True)

        load_model_button = gr.Button("é‡æ–°åŠ è½½æ¨¡å‹")
        load_model_button.click(reinit_model, show_progress=True,
                                inputs=[llm_model, embedding_model, kb_select_dropdown,
                                        # llm_history_len, no_remote_model, use_ptuning_v2, use_lora,
                                        chatbot],
                                outputs=chatbot)

    demo.load(
        fn=refresh_kb_list,
        inputs=None,
        outputs=kb_select_dropdown,
        queue=True,
    )

(demo
 .queue(concurrency_count=3)
 .launch(server_name='0.0.0.0',
         server_port=7860,
         show_api=False,
         share=False,
         inbrowser=False))
